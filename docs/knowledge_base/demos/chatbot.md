This demo showcases Taipy's ability to enable end-users to run inference using LLMs. Here, we
use GPT-3 to create a chatbot and display the conversation in an interactive chat interface.

[Try it live](https://demo-llm-chat.taipy.cloud/){: .tp-btn target='blank' }
[Get it on GitHub](https://github.com/Avaiga/demo-llm-chat){: .tp-btn .tp-btn--accent target='blank' }

# Understanding the Application

This application allows the user to chat with [GPT-3](https://openai.com/blog/gpt-3-apps) by sending 
its input to the OpenAI API and returning the conversation in 
a chat window. The user can also return to a previous 
conversation and continue it.

![ChatBot](images/chatbot_meds_conv.png){width=100%}

A tutorial on how to write this application and similar 
LLM inference applications is available [here](../tutorials/chatbot/index.md).